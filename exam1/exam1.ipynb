{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/en/5/5f/Western_Institute_of_Technology_and_Higher_Education_logo.png)\n",
    "\n",
    "**InstitutoTecnológico y de Estudios Superiores de Occidente**\n",
    "\n",
    "**Maestría Ciencia de Datos**\n",
    "\n",
    "**Aprendizaje Profundo**\n",
    "\n",
    "# Examen 1 #\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* * *\n",
    "\n",
    "Estudiante: Daniel Nuño <br>\n",
    "Profesor: Dr. Francisco Cervantes <br>\n",
    "Fecha entrega: Febrero 27, 2023 <br>\n",
    "\n",
    "* * *\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Generación de conjuntos de datos de entrenamiento, evaluación y pruebas \n",
    "\n",
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "main_folder_path = 'C:/Users/nuno/Desktop/deep-learning-data/exam1/' + 'Train/'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "counter = 0\n",
    "metadata_img = []\n",
    "r_size = 256\n",
    "arr = np.zeros((758, r_size, r_size, 3))\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(main_folder_path):\n",
    "        for filename in filenames:\n",
    "            dirs_paths_state = [str(os.path.basename(dirpath))]\n",
    "            metadata_img.append(dirs_paths_state) #Y array\n",
    "            img = Image.open(os.path.join(dirpath, filename)) #open image\n",
    "            if img.getbands() != ('RGB',):\n",
    "                img = img.convert('RGB')\n",
    "            img = img.resize((r_size, r_size))\n",
    "\n",
    "            arr[counter] = np.array(img)/255 #X array\n",
    "            counter += 1\n",
    "\n",
    "img.close()\n",
    "\n",
    "X_train_ = arr.astype(np.float16)\n",
    "y_train_ = np.array(metadata_img, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "path = '/content/drive/MyDrive/DL2023p-licd/S03/ejemplos/myDataGenerator/data_idg'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(28, 28),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        path,\n",
    "        target_size=(28, 28),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder_path = 'C:/Users/nuno/Desktop/deep-learning-data/exam1/' + 'Test/'\n",
    "\n",
    "counter = 0\n",
    "metadata_img = []\n",
    "r_size = 256\n",
    "arr = np.zeros((75, r_size, r_size, 3))\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(main_folder_path):\n",
    "        for filename in filenames:\n",
    "            dirs_paths_state = [str(os.path.basename(dirpath))]\n",
    "            metadata_img.append(dirs_paths_state) #Y array\n",
    "            img = Image.open(os.path.join(dirpath, filename)) #open image\n",
    "            if img.getbands() != ('RGB',):\n",
    "                img = img.convert('RGB')\n",
    "            img = img.resize((r_size, r_size))\n",
    "            arr[counter] = np.array(img)/255 #X array\n",
    "            counter += 1\n",
    "\n",
    "img.close()\n",
    "\n",
    "X_test = arr.astype(np.float16)\n",
    "y_test = np.array(metadata_img, dtype=str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "dummy_encoder_state = OneHotEncoder(sparse=False)\n",
    "dummy_encoder_state.fit(y_train_.reshape(-1,1))\n",
    "y_train_enc = dummy_encoder_state.transform(y_train_.reshape(-1,1))\n",
    "y_test_enc = dummy_encoder_state.transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((720, 256, 256, 3), (720, 6))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train_, y_train_enc, test_size=0.05, random_state=27)\n",
    "(X_train.shape, y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- balanceo de datos\n",
    "- generación de imagenes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Configuración de red neuronal basada en el modelo preentrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import Xception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(256,256,3))\n",
    "xception = Xception(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "xception.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_xception = xception.output\n",
    "\n",
    "x_tensor = Flatten()(output_xception)\n",
    "output_tensor = Dense(6, name = \"softmax\")(x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Resultados de entrenamiento y validación del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23/23 [==============================] - 74s 3s/step - loss: 2.3788 - accuracy: 0.0569\n",
      "Epoch 2/2\n",
      "23/23 [==============================] - 66s 3s/step - loss: 2.2782 - accuracy: 0.0444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2856f4e0ee0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "mymodel.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 - 8s - loss: 1.9763 - accuracy: 0.0533 - 8s/epoch - 3s/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = mymodel.evaluate(X_test, y_test_enc, verbose=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Reflexión ética sobre el posible uso dual, falta de consentimiento informado, protección de sujetos vulnerables, y proteción de datos.\n",
    "\n",
    "Aunque posiblemente el uso de estas imágenes fue consensuado para el investigador, puede que no haya sido otorgado a terceros, como es nuestro caso. El uso incorrecto de estas tecnologías y el reconocimiento fácil pone en peligro a los niños. Como de gobiernos que vigilan a sus ciudadanos o delincuentes que puedan extorsionar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "102a976f0e86ebaac5d44f1c7df09d65d6c14d74209e264d889fb01976a5cf38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
