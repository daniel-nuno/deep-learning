{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/en/5/5f/Western_Institute_of_Technology_and_Higher_Education_logo.png)\n",
    "\n",
    "**InstitutoTecnológico y de Estudios Superiores de Occidente**\n",
    "\n",
    "**Maestría Ciencia de Datos**\n",
    "\n",
    "**Aprendizaje Profundo**\n",
    "\n",
    "# Proyecto: clasificación y localización de objetos #\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* * *\n",
    "\n",
    "Estudiante: Daniel Nuño <br>\n",
    "Profesor: Dr. Francisco Cervantes <br>\n",
    "Fecha entrega: Marzo 26, 2023 <br>\n",
    "\n",
    "* * *\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2 as cv\n",
    "\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.data import AUTOTUNE\n",
    "\n",
    "#from tensorflow.keras.layers import Conv2D, Flatten, Dense, Input\n",
    "#from tensorflow.keras.applications import VGG16\n",
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing data and functions\n",
    "\n",
    "### Processing training data\n",
    "\n",
    "Set paths for training and ids-categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path= \"C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/train\"\n",
    "\n",
    "project_id_path = \"C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/wnids.txt\"\n",
    "all_id_cat_path = \"C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/words.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read project ids as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id_list = []\n",
    "with open(project_id_path) as f:\n",
    "    for line in f:\n",
    "        project_id_list.append(line.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read categories as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cat_dict = dict()\n",
    "with open(all_id_cat_path, 'r') as f:\n",
    "    for line in f:\n",
    "        resulting_line = line.strip().split('\\t')\n",
    "        id_cat_dict[resulting_line[0]] = resulting_line[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all list of files. Separate bounding box files frome images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files_img = []\n",
    "training_files_bb = []\n",
    "for dirpath, dirnames, filenames in os.walk(img_path):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirpath, filename)\n",
    "        if path.endswith('txt'):\n",
    "            training_files_bb.append(path)\n",
    "        else:\n",
    "            training_files_img.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 100000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_files_bb), len(training_files_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_dict = dict()\n",
    "for file in training_files_bb:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            img_name, xmin, ymin, xmax, ymax = line.strip().split('\\t')\n",
    "            bb_dict[img_name] = [xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check elements of dictionary of bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(bb_dict.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data set list that returns img full path, category, bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = []\n",
    "for file in training_files_img:\n",
    "    #get category and file name\n",
    "    _, category, _, image_name = training_files_img[0].split('\\\\')\n",
    "    #open image\n",
    "    img = cv.imread(file)\n",
    "    #get dimensions\n",
    "    h, w, _ = img.shape\n",
    "    #get correct size bounding box\n",
    "    original_bb = bb_dict[image_name]\n",
    "    rs_bb = [float(original_bb[0])/w,\n",
    "               float(original_bb[1])/h,\n",
    "               float(original_bb[2])/w,\n",
    "               float(original_bb[3])/h,\n",
    "                ]\n",
    "    # treat it as list\n",
    "    example = (file, category, rs_bb)\n",
    "    #appended to final list\n",
    "    training_list.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing validation data\n",
    "\n",
    "Path of images and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_val = \"C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/val/images\"\n",
    "val_annotations_txt = \"C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/val/val_annotations.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dict_val = dict()\n",
    "validation_list = list()\n",
    "with open(val_annotations_txt, 'r') as f:\n",
    "    for line in f:\n",
    "        img_name, category, xmin, ymin, xmax, ymax = line.strip().split('\\t')\n",
    "        full_path = img_path_val + '/' + img_name\n",
    "        img = cv.imread(full_path)\n",
    "        h, w, _ = img.shape\n",
    "        rs_xmin = float(xmin)/w\n",
    "        rs_xmax = float(xmax)/w\n",
    "        rs_ymin = float(ymin)/h\n",
    "        rs_ymax = float(ymax)/h\n",
    "        annotations_dict_val[full_path] = (category, [rs_xmin, rs_ymin, rs_xmax, rs_ymax])\n",
    "        validation_list.append((full_path, category, [rs_xmin, rs_ymin, rs_xmax, rs_ymax]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check training and validation have the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/train\\\\n04417672\\\\images\\\\n04417672_436.JPEG',\n",
       " 'n01443537',\n",
       " [0.0, 0.15625, 0.984375, 0.90625]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/val/images/val_0.JPEG',\n",
       " 'n03444034',\n",
       " [0.0, 0.5, 0.6875, 0.96875])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(validation_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'n03444034'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(validation_list[0][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load function\n",
    "\n",
    "Because we want to use TensorFlow batches, it is important to use TensorFlow classes. TensorFlow batches allow you to load data in small groups, instead of loading all into memory at once.\n",
    "\n",
    "The following function load and treat the image element by element. Each element is a list of three: [full image name, category, bounding box]\n",
    "\n",
    "bounding box is a list of 4: x_min, y_min, x_max, y_max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_element(element):\n",
    "    #load image\n",
    "    img = tf.io.read_file(element[0])\n",
    "    #make sure is 3 channels\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    #conver to float\n",
    "    img = tf.image.convert_image_dtype(img, dtype=tf.float16)\n",
    "    #resize\n",
    "    img = tf.image.resize(img, (128, 128))\n",
    "    #category\n",
    "    category = tf.constant(element[1])\n",
    "    #bounding box\n",
    "    x_min = tf.abs(element[2][0])\n",
    "    y_min = tf.abs(element[2][1])\n",
    "    x_max = tf.abs(element[2][2])\n",
    "    y_max = tf.abs(element[2][3])\n",
    "    bb = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    return (img, category, bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, category, bb = load_element(training_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 128, 3), dtype=float32, numpy=\n",
       "array([[[0.94921875, 0.94921875, 0.94921875],\n",
       "        [0.9511719 , 0.9511719 , 0.9511719 ],\n",
       "        [0.9550781 , 0.9550781 , 0.9550781 ],\n",
       "        ...,\n",
       "        [0.30096436, 0.30096436, 0.30096436],\n",
       "        [0.40093994, 0.40093994, 0.40093994],\n",
       "        [0.45092773, 0.45092773, 0.45092773]],\n",
       "\n",
       "       [[0.9511719 , 0.9511719 , 0.9511719 ],\n",
       "        [0.953125  , 0.953125  , 0.953125  ],\n",
       "        [0.95703125, 0.95703125, 0.95703125],\n",
       "        ...,\n",
       "        [0.3066101 , 0.3066101 , 0.3066101 ],\n",
       "        [0.35708618, 0.35708618, 0.35708618],\n",
       "        [0.38232422, 0.38232422, 0.38232422]],\n",
       "\n",
       "       [[0.9550781 , 0.9550781 , 0.9550781 ],\n",
       "        [0.95703125, 0.95703125, 0.95703125],\n",
       "        [0.9609375 , 0.9609375 , 0.9609375 ],\n",
       "        ...,\n",
       "        [0.3179016 , 0.3179016 , 0.3179016 ],\n",
       "        [0.26937866, 0.26937866, 0.26937866],\n",
       "        [0.24511719, 0.24511719, 0.24511719]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.15002441, 0.15002441, 0.15002441],\n",
       "        [0.16716766, 0.16716766, 0.16716766],\n",
       "        [0.20145416, 0.20145416, 0.20145416],\n",
       "        ...,\n",
       "        [0.6515808 , 0.6515808 , 0.6515808 ],\n",
       "        [0.67276   , 0.67276   , 0.67276   ],\n",
       "        [0.6833496 , 0.6833496 , 0.6833496 ]],\n",
       "\n",
       "       [[0.15197754, 0.15197754, 0.15197754],\n",
       "        [0.19950104, 0.19950104, 0.19950104],\n",
       "        [0.29454803, 0.29454803, 0.29454803],\n",
       "        ...,\n",
       "        [0.551178  , 0.551178  , 0.551178  ],\n",
       "        [0.5517273 , 0.5517273 , 0.5517273 ],\n",
       "        [0.55200195, 0.55200195, 0.55200195]],\n",
       "\n",
       "       [[0.1529541 , 0.1529541 , 0.1529541 ],\n",
       "        [0.21566772, 0.21566772, 0.21566772],\n",
       "        [0.34109497, 0.34109497, 0.34109497],\n",
       "        ...,\n",
       "        [0.50097656, 0.50097656, 0.50097656],\n",
       "        [0.49121094, 0.49121094, 0.49121094],\n",
       "        [0.48632812, 0.48632812, 0.48632812]]], dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'n01443537'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.15625>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.984375>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.90625>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define error (loss/metrics) function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for test and new data\n",
    "\n",
    "1. define folder path for images\n",
    "2. get images names\n",
    "3. recursively:\n",
    "    - load image\n",
    "    - resize\n",
    "    - get category and bounding box from nn\n",
    "    - save results to a list containing the image name, category and bounding box\n",
    "4. save list of results to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
