{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/en/5/5f/Western_Institute_of_Technology_and_Higher_Education_logo.png)\n",
    "\n",
    "**InstitutoTecnológico y de Estudios Superiores de Occidente**\n",
    "\n",
    "**Maestría Ciencia de Datos**\n",
    "\n",
    "**Aprendizaje Profundo**\n",
    "\n",
    "# Proyecto: clasificación y localización de objetos #\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* * *\n",
    "\n",
    "Estudiante: Daniel Nuño <br>\n",
    "Profesor: Dr. Francisco Cervantes <br>\n",
    "Fecha entrega: Marzo 26, 2023 <br>\n",
    "\n",
    "* * *\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing data and functions\n",
    "\n",
    "Because the folder structure for training is different than validation and test, the pre processing is differently.\n",
    "\n",
    "### Pre processing training data\n",
    "\n",
    "Set paths for training and ids-categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path= \"C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/train\"\n",
    "\n",
    "project_id_path = \"C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/wnids.txt\"\n",
    "all_id_cat_path = \"C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/words.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read project ids as list. Make sure it has unique values. Make and index of integers from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id_list = []\n",
    "with open(project_id_path) as f:\n",
    "    for line in f:\n",
    "        project_id_list.append(line.strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure it has unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(project_id_list)) == len(project_id_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an index of integers from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_index_dict = {value: index for index, value in enumerate(project_id_list)}\n",
    "project_index_dict_by_idx = {index: value for index, value in enumerate(project_id_list)}\n",
    "len(project_index_dict.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read file of categories as dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cat_dict = dict()\n",
    "with open(all_id_cat_path, 'r') as f:\n",
    "    for line in f:\n",
    "        resulting_line = line.strip().split('\\t')\n",
    "        id_cat_dict[resulting_line[0]] = resulting_line[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all list of files while separating bounding box files frome images. Check how many elements has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files_img = []\n",
    "training_files_bb = []\n",
    "for dirpath, dirnames, filenames in os.walk(img_path):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirpath, filename)\n",
    "        if path.endswith('txt'):\n",
    "            training_files_bb.append(path)\n",
    "        else:\n",
    "            training_files_img.append(path)\n",
    "\n",
    "len(training_files_bb), len(training_files_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_dict = dict()\n",
    "for file in training_files_bb:\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            img_name, xmin, ymin, xmax, ymax = line.strip().split('\\t')\n",
    "            bb_dict[img_name] = [xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check elements of dictionary of bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(bb_dict.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data set list that returns img full path, category, bounding box. Double back slash **'\\\\'** might needed to be updated depending on the users system.\n",
    "\n",
    "In tensorflow you can't have a tensor with more than one data type (tf.data.Dataset.from_tensor_slices function). Hence a workaround could be to create a tensor with data type tf.String and, on the occurrence (load_element function below), cast the field to the desired data type.\n",
    "\n",
    "> It is not possible to have a tf.Tensor with more than one data type. It is possible, however, to serialize arbitrary data structures as strings and store those in tf.Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list = []\n",
    "for count, file in enumerate(training_files_img):\n",
    "    #get category and file name\n",
    "    _, category_key, _, image_name = training_files_img[count].split('\\\\')\n",
    "    #convert category to index from dictionary\n",
    "    category = project_index_dict[category_key]\n",
    "    #open image\n",
    "    img = cv.imread(file)\n",
    "    #get dimensions\n",
    "    h, w, _ = img.shape\n",
    "    #get normalized size bounding box\n",
    "    original_bb = bb_dict[image_name]\n",
    "    rs_bb = [float(original_bb[0])/w,\n",
    "            float(original_bb[1])/h,\n",
    "            float(original_bb[2])/w,\n",
    "            float(original_bb[3])/h,\n",
    "            ]\n",
    "    # treat it as list\n",
    "    #example = (file, category, rs_bb)\n",
    "    #treat it as one string delimited by comas\n",
    "    example = \"\".join([file, ',',\n",
    "                        str(category), ',',\n",
    "                        str(rs_bb[0]), ',',\n",
    "                        str(rs_bb[1]), ',',\n",
    "                        str(rs_bb[2]), ',',\n",
    "                        str(rs_bb[3])])\n",
    "    #appended to final list\n",
    "    training_list.append(example)\n",
    "\n",
    "len(training_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing validation data\n",
    "\n",
    "Path of images and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_val = \"C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/val/images\"\n",
    "val_annotations_txt = \"C:/Users/nuno/Desktop/deep-learning-data/proyecto1/tiny-imagenet-200/val/val_annotations.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process annotations that contains category and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_list = list()\n",
    "with open(val_annotations_txt, 'r') as f:\n",
    "    for line in f:\n",
    "        img_name, category_key, xmin, ymin, xmax, ymax = line.strip().split('\\t')\n",
    "        full_path = img_path_val + '/' + img_name\n",
    "        category = project_index_dict[category_key]\n",
    "        img = cv.imread(full_path)\n",
    "        h, w, _ = img.shape\n",
    "        rs_xmin = float(xmin)/w\n",
    "        rs_ymin = float(ymin)/h\n",
    "        rs_xmax = float(xmax)/w\n",
    "        rs_ymax = float(ymax)/h\n",
    "        rs_bb = [rs_xmin, rs_ymin, rs_xmax, rs_ymax]\n",
    "        #treat it as list\n",
    "        #example = (full_path, category, rs_bb)\n",
    "        #treat it as one string delimited by comas\n",
    "        example = \"\".join([full_path, ',',\n",
    "                        str(category), ',',\n",
    "                        str(rs_bb[0]), ',',\n",
    "                        str(rs_bb[1]), ',',\n",
    "                        str(rs_bb[2]), ',',\n",
    "                        str(rs_bb[3])])\n",
    "        validation_list.append(example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check training and validation have the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_list[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load function and dataset\n",
    "\n",
    "Because we want to use TensorFlow in batches, it is important to use TensorFlow classes. TensorFlow Dataset allows to load data in small groups, instead of loading all into memory at once.\n",
    "\n",
    "> Supports writing descriptive and efficient input pipelines. Dataset usage follows a common pattern:\n",
    "\n",
    "> - Create a source dataset from your input data.\n",
    "> - Apply dataset transformations to preprocess the data.\n",
    ">- Iterate over the dataset and process the elements.\n",
    "\n",
    "> Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory.\n",
    "\n",
    "The following function load and treat the image element by element. To be used in the pre-fetched function. Each element is a string delimited by comas:\n",
    "- full image name, category, ,x min, y min, x max, y max.\n",
    "\n",
    "- category is an integer from indexing. To use Sparse Categorical Cross Entropy as loss. Expexted to save time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector. Activation function use softmax and neurons is num_classes. y_pred is [batch_size, num_classes].\n",
    "- bounding box is a list of 4 float numbers: x_min, y_min, x_max, y_max. Activation function use linear and neurons is 4. y_pred is [batch_size, 4].\n",
    "\n",
    "The function return a pair on input and targets (category and bounding box). This format is required for *model fit method*:\n",
    "\n",
    "> Args\n",
    "> x: Input data. It could be: ...A tf.data dataset. Should return a tuple of either (inputs, targets) or (inputs, targets, sample_weights)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_element(element):\n",
    "    #make tensors list delimited by ,\n",
    "    element = tf.strings.split(element, sep=\",\")\n",
    "    #load image\n",
    "    img = tf.io.read_file(element[0])\n",
    "    #make sure is 3 channels\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    #conver to float [0,1)\n",
    "    #img = tf.image.convert_image_dtype(img, dtype=tf.float16)\n",
    "    #resize\n",
    "    img = tf.image.resize(img, (128, 128))\n",
    "    #category\n",
    "    #category = tf.constant(element[1])\n",
    "    category =tf.strings.to_number(element[1], tf.int32)\n",
    "    #bounding box\n",
    "    x_min = tf.strings.to_number(element[2])\n",
    "    y_min = tf.strings.to_number(element[3])\n",
    "    x_max = tf.strings.to_number(element[4])\n",
    "    y_max = tf.strings.to_number(element[5])\n",
    "    #bb = [y_min, x_min, y_max, x_max]\n",
    "    bb = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    labels = {'class_output': category, 'box_output':bb}\n",
    "\n",
    "    return (img, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, labels = load_element(training_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function **tf.data.Dataset** represents a potentially large set of elements. Iteration in training streaming fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bath_size = 16\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(training_list)\n",
    "train_dataset = (train_dataset\n",
    "                 .shuffle(len(training_list))\n",
    "                 .map(load_element, num_parallel_calls=AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .batch(bath_size)\n",
    "                 .prefetch(AUTOTUNE)\n",
    "                 )\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices(validation_list)\n",
    "val_dataset = (val_dataset\n",
    "                 .shuffle(len(validation_list))\n",
    "                 .map(load_element, num_parallel_calls = AUTOTUNE)\n",
    "                 .cache()\n",
    "                 .batch(bath_size)\n",
    "                 .prefetch(AUTOTUNE)\n",
    "                 )\n",
    "\n",
    "val_dataset.element_spec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define error (loss/metrics) function\n",
    "\n",
    "### Generalized intersection over union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_GIoU(bb_true, bb_pred):\n",
    "    #make zero as tensor\n",
    "    zero = tf.convert_to_tensor(0.0, bb_true.dtype)\n",
    "    #convert them to tensor clases\n",
    "    Ax1, Ay1, Ax2, Ay2 = tf.unstack(bb_true, 4, axis=-1)\n",
    "    Bx1, By1, Bx2, By2 = tf.unstack(bb_pred, 4, axis=-1)\n",
    "\n",
    "    #for the bounding box predicted make sure Bx2 > Bx1 y By2 > By1\n",
    "    bx1 = tf.math.minimum(Bx1, Bx2)\n",
    "    by1 = tf.math.minimum(By1, By2)\n",
    "    bx2 = tf.math.maximum(Bx1, Bx2)\n",
    "    by2 = tf.math.maximum(By1, By2)\n",
    "\n",
    "    #calculate area of true bounding box\n",
    "    A_area = (Ax2 - Ax1)*(Ay2 - Ay1)\n",
    "    #calculate area of predicted bounding box\n",
    "    B_area = (bx2 - bx1)*(by2 - by1)\n",
    "    \n",
    "    #calculate intersection over true and pred\n",
    "    #find the box overlaps both boxes\n",
    "    #each inter calculates the smallest stride\n",
    "    x_inter_1 = tf.math.maximum(bx1, Ax1)\n",
    "    y_inter_1 = tf.math.maximum(by1, Ay1)\n",
    "    x_inter_2 = tf.math.minimum(bx2, Ax2)\n",
    "    y_inter_2 = tf.math.minimum(by2, Ay2)\n",
    "    #get width\n",
    "    w_inter = tf.maximum(zero, x_inter_2 - x_inter_1)\n",
    "    #get height\n",
    "    h_inter = tf.maximum(zero, y_inter_2 - y_inter_1)\n",
    "    #intersection\n",
    "    I = w_inter * h_inter\n",
    "    #area over union\n",
    "    area_union = (B_area + A_area) - I\n",
    "    iou = tf.math.divide_no_nan(I, area_union)\n",
    "\n",
    "    #find the b box C smaller that surrounding/fits both A and B\n",
    "    Cx1 = tf.math.minimum(bx1, Ax1)\n",
    "    Cy1 = tf.math.minimum(by1, Ay1)\n",
    "    Cx2 = tf.math.maximum(bx2, Ax2)\n",
    "    Cy2 = tf.math.maximum(by2, Ay2)\n",
    "\n",
    "    #calculate the C area\n",
    "    C_area = (Cx2 - Cx1) * (Cy2 - Cy1)\n",
    "    #calculate giou\n",
    "    giou = iou - tf.math.divide_no_nan(C_area - area_union, C_area)\n",
    "    #calculate mean of all observations\n",
    "    m_giou = tf.reduce_mean(giou, axis=0)\n",
    "\n",
    "    return m_giou\n",
    "\n",
    "def my_GIoULoss(bb_true, bb_pred):\n",
    "    return 1.0 - my_GIoU(bb_true, bb_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom classification accuracy error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sparse_category_accurary(y_true, y_pred):\n",
    "    #y_true is expected to be integer\n",
    "    #y_pred is a numpy.ndarray with the normalized probabilities\n",
    "    acc = np.dot(1, np.not_equal(y_true, np.argmax(y_pred, axis=1)))\n",
    "    return acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom bounding box IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_IoU_np(y_true, y_pred):\n",
    "    #becausse model predict returns a numpy.ndarray the convert to tensors because le function already works a tensors\n",
    "    bb_true = tf.convert_to_tensor(y_true, y_true.dtype)\n",
    "    bb_pred = tf.convert_to_tensor(y_pred, y_pred.dtype)\n",
    "\n",
    "    #make zero as tensor\n",
    "    zero = tf.convert_to_tensor(0.0, bb_true.dtype)\n",
    "    #convert them to tensor clases\n",
    "    Ax1, Ay1, Ax2, Ay2 = tf.unstack(bb_true, 4, axis=-1)\n",
    "    Bx1, By1, Bx2, By2 = tf.unstack(bb_pred, 4, axis=-1)\n",
    "\n",
    "    #for the bounding box predicted make sure Bx2 > Bx1 y By2 > By1\n",
    "    bx1 = tf.math.minimum(Bx1, Bx2)\n",
    "    by1 = tf.math.minimum(By1, By2)\n",
    "    bx2 = tf.math.maximum(Bx1, Bx2)\n",
    "    by2 = tf.math.maximum(By1, By2)\n",
    "\n",
    "    #calculate area of true bounding box\n",
    "    A_area = (Ax2 - Ax1)*(Ay2 - Ay1)\n",
    "    #calculate area of predicted bounding box\n",
    "    B_area = (bx2 - bx1)*(by2 - by1)\n",
    "    \n",
    "    #calculate intersection over true and pred\n",
    "    #find the box overlaps both boxes\n",
    "    #each inter calculates the smallest stride\n",
    "    x_inter_1 = tf.math.maximum(bx1, Ax1)\n",
    "    y_inter_1 = tf.math.maximum(by1, Ay1)\n",
    "    x_inter_2 = tf.math.minimum(bx2, Ax2)\n",
    "    y_inter_2 = tf.math.minimum(by2, Ay2)\n",
    "    #get width\n",
    "    w_inter = tf.maximum(zero, x_inter_2 - x_inter_1)\n",
    "    #get height\n",
    "    h_inter = tf.maximum(zero, y_inter_2 - y_inter_1)\n",
    "    #intersection\n",
    "    I = w_inter * h_inter\n",
    "    #area over union\n",
    "    area_union = (B_area + A_area) - I\n",
    "    iou = tf.math.divide_no_nan(I, area_union)\n",
    "\n",
    "    #convert back to numpy\n",
    "    iou = iou.numpy()\n",
    "    #check if is over 50% then 0 otherwise 1\n",
    "    over_50 = np.where(iou>0.5, 0, 1)\n",
    "\n",
    "    return over_50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine both to return a correct match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_error(y_true_cat, y_pred_cat, y_true_bb, y_pred_bb):\n",
    "    d = my_sparse_category_accurary(y_true_cat, y_pred_cat)\n",
    "    f = my_IoU_np(y_true_bb, y_pred_bb)\n",
    "    e = np.max(np.column_stack((d,f)), axis=1)\n",
    "    return e, e.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Models that are pre trained already did the heavy lifting on getting a tested architecture and fine tuned parameters. Premade architectures with pre-trained weights.\n",
    "\n",
    "For EfficientNetV2, by default input preprocessing is included as a part of the model (as a Rescaling layer), and thus tf.keras.applications.efficientnet_v2.preprocess_input is actually a pass-through function. In this use case, EfficientNetV2 models expect their inputs to be float tensors of pixels with values in the [0-255] range. At the same time, preprocessing as a part of the model (i.e. Rescaling layer) can be disabled by setting include_preprocessing argument to False. With preprocessing disabled EfficientNetV2 models expect their inputs to be float tensors of pixels with values in the [-1, 1] range.\n",
    "\n",
    "### Load architecture\n",
    "\n",
    "We’ll first download the model but exclude the top since we’ll be adding our own custom top and resize the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.EfficientNetV2M(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=Input(shape=(128,128,3)),\n",
    "    pooling=None,\n",
    "    include_preprocessing=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define rest of the multi-output mode for the two tasks\n",
    "\n",
    "We won’t be training from scratch so we will use transfer learning and split output in two.\n",
    "\n",
    "- class_output\n",
    "- box_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable=False\n",
    "base_model_output = base_model.output\n",
    "\n",
    "no_of_classes = 200\n",
    "\n",
    "# We could also use Flatten()(x) but GAP is more effective,\n",
    "# it reduces parameters and controls overfitting.\n",
    "flattened_output = GlobalAveragePooling2D()(base_model_output)\n",
    "\n",
    "# Create our Classification Head, final layer contains \n",
    "# Ouput units = no. classes\n",
    "class_prediction = Dense(256, activation=\"relu\")(flattened_output)\n",
    "class_prediction = Dropout(0.2)(class_prediction )\n",
    "class_prediction = Dense(no_of_classes, activation='softmax',name=\"class_output\")(class_prediction)\n",
    "\n",
    "# Create Our Localization Head, final layer contains 4 nodes for x1,y1,x2,y2\n",
    "# Respectively.\n",
    "box_output = Dense(256, activation=\"relu\")(flattened_output)\n",
    "box_output = Dense(128, activation=\"relu\")(box_output)\n",
    "box_output = Dropout(0.2)(box_output)\n",
    "box_output = Dense(64, activation=\"relu\")(box_output)\n",
    "box_output = Dropout(0.2)(box_output)\n",
    "box_output = Dense(32, activation=\"relu\")(box_output)\n",
    "box_predictions = Dense(4, activation='sigmoid', name= \"box_output\")(box_output)\n",
    "\n",
    "# Now combine the two heads\n",
    "model = Model(inputs=base_model.input, outputs=[class_prediction, box_predictions])\n",
    "\n",
    "model.outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification we will have sparse categorical crossentropy\n",
    "# For the bouding boxes we will have GIoU\n",
    "losses = { \n",
    "    \"class_output\": tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \"box_output\": my_GIoULoss\n",
    "    }\n",
    "\n",
    "# For the class labels we want to know the Accuracy\n",
    "# And for the bounding boxes we need to know GIoU\n",
    "metrics = {\n",
    "    'class_output': tf.keras.metrics.SparseCategoricalAccuracy(), \n",
    "    'box_output': my_GIoU\n",
    "    }\n",
    "\n",
    "model.compile(optimizer='adam', loss=losses, metrics=metrics)\n",
    "\n",
    "stop = EarlyStopping(monitor = \"val_loss\", min_delta = 0.001, patience = 40, \n",
    "                    restore_best_weights = True\n",
    "                     )\n",
    "\n",
    "model_history = model.fit(x=train_dataset, validation_data=val_dataset, epochs=5, callbacks=stop, verbose=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with custom error\n",
    "\n",
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = reconstructed_model.predict(val_dataset)\n",
    "y_pred_cat, y_pred_bb = predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get true values from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_cat = np.array([-1])\n",
    "y_true_bb = np.array([-1, -1, -1, -1])\n",
    "for element in val_dataset.as_numpy_iterator():\n",
    "    #y_true_cat.insert(element[1]['class_output'])\n",
    "    #y_true_bb.insert(element[1]['box_output'])\n",
    "\n",
    "    y_true_cat = np.append(y_true_cat, element[1]['class_output'])\n",
    "    y_true_bb = np.append(y_true_bb, element[1]['box_output'])\n",
    "\n",
    "y_true_cat = y_true_cat[1:]\n",
    "y_true_bb = y_true_bb[4:]\n",
    "\n",
    "y_true_bb = y_true_bb.reshape((100,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, f, e, e_media = custom_error(y_true_cat, y_pred_cat, y_true_bb, y_pred_bb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is and error, to get an accuracy then use $1-e$ of the model as described in the intructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - e_media"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and validation data pipeline\n",
    "\n",
    "### Reconstruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_model = tf.keras.models.load_model('model', custom_objects={'my_GIoULoss':my_GIoULoss, 'my_GIoU':my_GIoU})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for test and new data\n",
    "\n",
    "1. Define folder path for images\n",
    "2. Get images names\n",
    "3. Recursively and for batches:\n",
    "    - Load image\n",
    "    - Convert to rgb.\n",
    "    - Resize\n",
    "4. make predictions\n",
    "5. Get category and bounding box from model\n",
    "    - Get category from dictionary. Dictionary in the form {1:'n02124075'}\n",
    "    - Convert from normalized to actual size bounding box\n",
    "6. Save results to a list txt containing the image name, category and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_from_folder(path, model, batch_size):\n",
    "    #2\n",
    "    img_list = []\n",
    "    for dirpath, dirnames, filenames in os.walk(img_path):\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(dirpath, filename)\n",
    "            img_list.append(path)\n",
    "    #3\n",
    "    def load_test_element(element):\n",
    "        #load img\n",
    "        img = tf.io.read_file(element)\n",
    "        #make sure is 3 channels\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        #resize\n",
    "        img = tf.image.resize(img, (128, 128))\n",
    "        return img\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(img_list)\n",
    "    test_dataset = (test_dataset\n",
    "                    .map(load_test_element, num_parallel_calls=AUTOTUNE)\n",
    "                    .cache()\n",
    "                    .batch(batch_size)\n",
    "                    .prefetch(AUTOTUNE)\n",
    "                    )\n",
    "    #4\n",
    "    test_predictions = model.predict(test_dataset)\n",
    "    cat_pred, bb_pred = test_predictions\n",
    "    #5.1\n",
    "    converted_pred_cat = []\n",
    "    for count, pred in enumerate(cat_pred):\n",
    "        arg_max = np.argmax(pred)\n",
    "        converted_pred_cat.append(arg_max)\n",
    "\n",
    "    return img_list, converted_pred_cat, bb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(img_list, cat_pred, bb_pred, dict_cat, file_name):\n",
    "    #cat_pred is a list or numpy array of integers\n",
    "    #bb_pred in a list or numpy array of numpy arrays of 4\n",
    "    converted_pred_cat = []\n",
    "    for count, pred in enumerate(cat_pred):\n",
    "        converted_pred_cat.append(dict_cat[pred])\n",
    "    #5.2\n",
    "    converted_pred_bb = []\n",
    "    for count, pred in enumerate(bb_pred):\n",
    "        #get image path\n",
    "        file = img_list[count]\n",
    "        #get real image size\n",
    "        img = cv.imread(file)\n",
    "        #get dimensions\n",
    "        h, w, _ = img.shape\n",
    "        #correct size bb\n",
    "        x_min = str(round(pred[0]*w))\n",
    "        y_min = str(round(pred[1]*h))\n",
    "        x_max = str(round(pred[2]*w))\n",
    "        y_max = str(round(pred[3]*h))\n",
    "        #make a string list\n",
    "        string = \"\".join([x_min, '\\t',\n",
    "                        y_min, '\\t',\n",
    "                        x_max, '\\t',\n",
    "                        y_max])\n",
    "        converted_pred_bb.append(string)\n",
    "    #6\n",
    "    output_list = []\n",
    "    with open(file_name, 'w') as f:\n",
    "        for count, file in enumerate(img_list):\n",
    "            _, image_name = file.split('\\\\')\n",
    "            output = \"\".join([image_name, '\\t',\n",
    "                            converted_pred_cat[count], '\\t',\n",
    "                            converted_pred_bb[count]])\n",
    "            output_list.append(output)\n",
    "            f.write(output)\n",
    "            f.write('\\n')\n",
    "\n",
    "    return output_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
